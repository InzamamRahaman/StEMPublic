{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dataloaders\n",
    "import models\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.metrics as metrics \n",
    "import sklearn.cluster as cluster\n",
    "import numpy as np \n",
    "import random\n",
    "import classifiers\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import sklearn.model_selection as model_selection\n",
    "import util\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import pathlib\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_sample(X, y, ratio=0.2):\n",
    "    if ratio != 1.0:\n",
    "        num_elems = len(X)\n",
    "        indicies = list(range(0, num_elems))\n",
    "        sampled_indicies = random.sample(indicies, int(ratio * num_elems))\n",
    "        sampled_indicies = np.array(sampled_indicies)\n",
    "        X_sampled = X[sampled_indicies,:]\n",
    "        y_sampled = y[sampled_indicies]\n",
    "        return X_sampled, y_sampled\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_per = 1\n",
    "sample_ratios = [0.6, 0.8, 1.0]\n",
    "num_splits = 5\n",
    "first = 102\n",
    "last = 107\n",
    "house_sitting_numbers = range(first, last + 1)\n",
    "input_data_path = pathlib.Path('../data/house')\n",
    "delimiter = ','"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# paramaters shared by all models\n",
    "num_nodes = data.get_num_nodes()\n",
    "dims = 8\n",
    "epochs = 50\n",
    "lr = 0.1\n",
    "lr_decay=0.0\n",
    "weight_decay=0.0\n",
    "lam = 0.00055\n",
    "p = 2\n",
    "\n",
    "# parameters specific to SiNE\n",
    "delta = 1\n",
    "delta0 = 0.5\n",
    "dims_array = [dims, 20, 20]\n",
    "frac1 = 0.3\n",
    "frac0 = 0\n",
    "p0 = True if frac0 > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Considering house #102 dataset\n",
      "Processing 0.6 of samples\n",
      "Len of original: 194040, 194040\n",
      "Len of sampled: 116424, 116424\n",
      "The loss at epoch  1  was  0.7017831802368164\n",
      "The loss at epoch  2  was  0.6924053430557251\n",
      "The loss at epoch  3  was  0.6797216534614563\n",
      "The loss at epoch  4  was  0.650488555431366\n",
      "The loss at epoch  5  was  0.5793423652648926\n",
      "The loss at epoch  6  was  0.499355286359787\n",
      "The loss at epoch  7  was  0.43605950474739075\n",
      "The loss at epoch  8  was  0.42542847990989685\n",
      "The loss at epoch  9  was  0.42194709181785583\n",
      "The loss at epoch  10  was  0.4192137122154236\n",
      "The loss at epoch  11  was  0.41616544127464294\n",
      "The loss at epoch  12  was  0.4133223295211792\n",
      "The loss at epoch  13  was  0.41123926639556885\n",
      "The loss at epoch  14  was  0.40967807173728943\n",
      "The loss at epoch  15  was  0.40794846415519714\n",
      "The loss at epoch  16  was  0.40630674362182617\n",
      "The loss at epoch  17  was  0.40413397550582886\n",
      "The loss at epoch  18  was  0.40180158615112305\n",
      "The loss at epoch  19  was  0.3990350067615509\n",
      "The loss at epoch  20  was  0.3959987759590149\n",
      "The loss at epoch  21  was  0.39256393909454346\n",
      "The loss at epoch  22  was  0.38884973526000977\n",
      "The loss at epoch  23  was  0.3850533962249756\n",
      "The loss at epoch  24  was  0.38147813081741333\n",
      "The loss at epoch  25  was  0.37839722633361816\n",
      "The loss at epoch  26  was  0.37601223587989807\n",
      "The loss at epoch  27  was  0.37407025694847107\n",
      "The loss at epoch  28  was  0.3738892078399658\n",
      "The loss at epoch  29  was  0.37089473009109497\n",
      "The loss at epoch  30  was  0.3705008327960968\n",
      "The loss at epoch  31  was  0.3685809075832367\n",
      "The loss at epoch  32  was  0.3687928020954132\n",
      "The loss at epoch  33  was  0.3666291832923889\n",
      "The loss at epoch  34  was  0.3669651746749878\n",
      "The loss at epoch  35  was  0.3643891513347626\n",
      "The loss at epoch  36  was  0.3643900156021118\n",
      "The loss at epoch  37  was  0.36305058002471924\n",
      "The loss at epoch  38  was  0.3642525374889374\n",
      "The loss at epoch  39  was  0.36174431443214417\n",
      "The loss at epoch  40  was  0.3622376620769501\n",
      "The loss at epoch  41  was  0.36013704538345337\n",
      "The loss at epoch  42  was  0.3610674738883972\n",
      "The loss at epoch  43  was  0.36016085743904114\n",
      "The loss at epoch  44  was  0.3605121374130249\n",
      "The loss at epoch  45  was  0.35795336961746216\n",
      "The loss at epoch  46  was  0.3581816554069519\n",
      "The loss at epoch  47  was  0.3577823042869568\n",
      "The loss at epoch  48  was  0.3579395115375519\n",
      "The loss at epoch  49  was  0.356622576713562\n",
      "The loss at epoch  50  was  0.3568466603755951\n",
      "Loss at epoch  1  is  1.0068774223327637\n",
      "Loss at epoch  2  is  0.984351634979248\n",
      "Loss at epoch  3  is  0.8564539551734924\n",
      "Loss at epoch  4  is  0.9980366826057434\n",
      "Loss at epoch  5  is  0.9720420837402344\n",
      "Loss at epoch  6  is  0.9201155304908752\n",
      "Loss at epoch  7  is  0.8905458450317383\n",
      "Loss at epoch  8  is  0.8988160490989685\n",
      "Loss at epoch  9  is  0.8494970202445984\n",
      "Loss at epoch  10  is  0.843043863773346\n",
      "Loss at epoch  11  is  0.8971897959709167\n",
      "Loss at epoch  12  is  0.8600093722343445\n",
      "Loss at epoch  13  is  0.8459926843643188\n",
      "Loss at epoch  14  is  0.8453623652458191\n",
      "Loss at epoch  15  is  0.8473961353302002\n",
      "Loss at epoch  16  is  0.8418448567390442\n",
      "Loss at epoch  17  is  0.8358122706413269\n",
      "Loss at epoch  18  is  0.8324474692344666\n",
      "Loss at epoch  19  is  0.8296676278114319\n",
      "Loss at epoch  20  is  0.8281546235084534\n",
      "Loss at epoch  21  is  0.8268015384674072\n",
      "Loss at epoch  22  is  0.8267471790313721\n",
      "Loss at epoch  23  is  0.8275156021118164\n",
      "Loss at epoch  24  is  0.8302562236785889\n",
      "Loss at epoch  25  is  0.8323051929473877\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-d18f0df4166e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m                 sine_model = models.fit_sine_model(num_nodes, dims_array, triples, triples0, delta, delta0,\n\u001b[1;32m     58\u001b[0m                                    \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_decay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                                   lam=lam, p=p, p0=p0)\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0mclf_sine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_sine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/SignEmb-3985b33de9ca5751ec59cf7b3596bf12bb298a21 copy/src/models.py\u001b[0m in \u001b[0;36mfit_sine_model\u001b[0;34m(num_nodes, dims_arr, triples, triples0, delta, delta0, batch_size, batch_size0, epochs, lr, lam, lr_decay, p, print_loss, p0)\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprint_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "results = defaultdict(dict)\n",
    "\n",
    "\n",
    "\n",
    "for house_num in [102]:\n",
    "    edgelist_file = f'house-{house_num}-cleaned.csv'\n",
    "    classes_file = f'house-{house_num}-classes.csv'\n",
    "    \n",
    "    print(f'Considering house #{house_num} dataset')\n",
    "    edgelist_path = input_data_path / edgelist_file\n",
    "    classes_path = input_data_path / classes_file\n",
    "    data = dataloaders.SenateDataset(str(edgelist_path), str(classes_path), ratio=0.9)\n",
    "    X, y = data.get_shuffled_data()\n",
    "    stem = defaultdict(list)\n",
    "    sine = defaultdict(list)\n",
    "    results[house_num]['stem'] = stem\n",
    "    results[house_num]['sine'] = sine\n",
    "    for sample_ratio in sample_ratios:\n",
    "        print(f'Processing {sample_ratio} of samples')\n",
    "        for i in range(num_samples_per):\n",
    "            \n",
    "            \n",
    "            X_sampled, y_sampled = random_sample(X, y, sample_ratio)\n",
    "            print(f'Len of original: {len(X)}, {len(y)}')\n",
    "            print(f'Len of sampled: {len(X_sampled)}, {len(y_sampled)}')\n",
    "            triples, triples0 = util.triples_from_array(X_sampled, y_sampled)\n",
    "            batch_size = int(frac1 * len(triples))\n",
    "            batch_size0 = int(frac0 * len(triples0))\n",
    "            kf = model_selection.KFold(n_splits=num_splits)\n",
    "            node_names = np.arange(0, num_nodes) + 1\n",
    "            node_classes = np.array(data.get_node_classes())\n",
    "            node_names = node_names[node_classes < 2]\n",
    "            node_classes = node_classes[node_classes < 2]\n",
    "            kf.get_n_splits(node_names)\n",
    "            \n",
    "            \n",
    "            for node_train_idx, node_test_idx in kf.split(node_names):\n",
    "                node_X_train, node_y_train = node_names[node_train_idx], node_classes[node_train_idx]\n",
    "                node_X_test, node_y_test = node_names[node_test_idx], node_classes[node_test_idx]\n",
    "                \n",
    "                # train and evaluate StEM model\n",
    "                kernel_model = models.fit_pseudo_kernel_model(num_nodes, dims, X_sampled, y_sampled, epochs=epochs, p=p, \n",
    "                                              lr=lr,lr_decay=lr_decay, lam=lam, \n",
    "                            weight_decay=weight_decay, undersample=False)\n",
    "                kernel_clf = linear_model.LogisticRegression()\n",
    "                clf = kernel_clf\n",
    "                model = kernel_model\n",
    "                report = classifiers.train_and_evaluate_node_classifier(clf, model, \n",
    "                                                                        node_X_train, node_y_train, \n",
    "                                                                        node_X_test, node_y_test)\n",
    "                stem[sample_ratio].append(report)\n",
    "                #print(report)\n",
    "                \n",
    "                # train and evaluate SiNE model\n",
    "                \n",
    "                sine_model = models.fit_sine_model(num_nodes, dims_array, triples, triples0, delta, delta0,\n",
    "                                   batch_size, batch_size0, epochs, lr=lr, lr_decay=lr_decay,\n",
    "                                  lam=lam, p=p, p0=p0)\n",
    "                clf_sine = linear_model.LogisticRegression()\n",
    "                clf = clf_sine\n",
    "                model = sine_model\n",
    "                report = classifiers.train_and_evaluate_node_classifier(clf, model, \n",
    "                                                                        node_X_train, node_y_train, \n",
    "                                                                        node_X_test, node_y_test)\n",
    "                sine[sample_ratio].append(report)\n",
    "                \n",
    "            print(results[house_num])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {102: {'sine': defaultdict(list,\n",
       "                          {0.2: [{'macro_f1': 0.86185243328100469,\n",
       "                             'micro_f1': 0.86363636363636365},\n",
       "                            {'macro_f1': 0.83238095238095233,\n",
       "                             'micro_f1': 0.86363636363636365},\n",
       "                            {'macro_f1': 0.7645484949832777,\n",
       "                             'micro_f1': 0.81818181818181823},\n",
       "                            {'macro_f1': 0.71652093879429368,\n",
       "                             'micro_f1': 0.76136363636363635},\n",
       "                            {'macro_f1': 0.81299332119004242,\n",
       "                             'micro_f1': 0.84090909090909094},\n",
       "                            {'macro_f1': 0.83396226415094343,\n",
       "                             'micro_f1': 0.84090909090909094},\n",
       "                            {'macro_f1': 0.8832375055285272,\n",
       "                             'micro_f1': 0.89772727272727271},\n",
       "                            {'macro_f1': 0.75325746330199572,\n",
       "                             'micro_f1': 0.80681818181818177},\n",
       "                            {'macro_f1': 0.69269841269841281,\n",
       "                             'micro_f1': 0.75},\n",
       "                            {'macro_f1': 0.56218905472636815,\n",
       "                             'micro_f1': 0.68181818181818177}],\n",
       "                           0.4: [{'macro_f1': 0.97697540554683404,\n",
       "                             'micro_f1': 0.97727272727272729},\n",
       "                            {'macro_f1': 0.90743801652892553,\n",
       "                             'micro_f1': 0.92045454545454553},\n",
       "                            {'macro_f1': 0.78787878787878785,\n",
       "                             'micro_f1': 0.84090909090909094},\n",
       "                            {'macro_f1': 0.86200997861724882,\n",
       "                             'micro_f1': 0.875},\n",
       "                            {'macro_f1': 0.85729028453486666,\n",
       "                             'micro_f1': 0.875},\n",
       "                            {'macro_f1': 0.98851624690069162,\n",
       "                             'micro_f1': 0.98863636363636365},\n",
       "                            {'macro_f1': 0.9362595972765464,\n",
       "                             'micro_f1': 0.94318181818181823},\n",
       "                            {'macro_f1': 0.85677083333333326,\n",
       "                             'micro_f1': 0.88636363636363646},\n",
       "                            {'macro_f1': 0.76428571428571423,\n",
       "                             'micro_f1': 0.79545454545454553},\n",
       "                            {'macro_f1': 0.57073170731707323,\n",
       "                             'micro_f1': 0.69318181818181823}],\n",
       "                           0.6: [{'macro_f1': 0.94258123450345821,\n",
       "                             'micro_f1': 0.94318181818181823},\n",
       "                            {'macro_f1': 0.75325746330199572,\n",
       "                             'micro_f1': 0.80681818181818177}]}),\n",
       "              'stem': defaultdict(list,\n",
       "                          {0.2: [{'macro_f1': 0.98856400259909027,\n",
       "                             'micro_f1': 0.98863636363636365},\n",
       "                            {'macro_f1': 0.95089285714285698,\n",
       "                             'micro_f1': 0.95454545454545459},\n",
       "                            {'macro_f1': 0.98725191945530932,\n",
       "                             'micro_f1': 0.98863636363636365},\n",
       "                            {'macro_f1': 0.93975078734766537,\n",
       "                             'micro_f1': 0.94318181818181823},\n",
       "                            {'macro_f1': 0.97628032345013471,\n",
       "                             'micro_f1': 0.97727272727272729},\n",
       "                            {'macro_f1': 0.98856400259909027,\n",
       "                             'micro_f1': 0.98863636363636365},\n",
       "                            {'macro_f1': 0.95089285714285698,\n",
       "                             'micro_f1': 0.95454545454545459},\n",
       "                            {'macro_f1': 0.97428404441846883,\n",
       "                             'micro_f1': 0.97727272727272729},\n",
       "                            {'macro_f1': 0.9642421779764323,\n",
       "                             'micro_f1': 0.96590909090909094},\n",
       "                            {'macro_f1': 0.97628032345013471,\n",
       "                             'micro_f1': 0.97727272727272729}],\n",
       "                           0.4: [{'macro_f1': 0.98856400259909027,\n",
       "                             'micro_f1': 0.98863636363636365},\n",
       "                            {'macro_f1': 0.9754464285714286,\n",
       "                             'micro_f1': 0.97727272727272729},\n",
       "                            {'macro_f1': 0.40136054421768708,\n",
       "                             'micro_f1': 0.67045454545454541},\n",
       "                            {'macro_f1': 0.93975078734766537,\n",
       "                             'micro_f1': 0.94318181818181823},\n",
       "                            {'macro_f1': 0.97628032345013471,\n",
       "                             'micro_f1': 0.97727272727272729},\n",
       "                            {'macro_f1': 0.98856400259909027,\n",
       "                             'micro_f1': 0.98863636363636365},\n",
       "                            {'macro_f1': 0.39310344827586208,\n",
       "                             'micro_f1': 0.64772727272727271},\n",
       "                            {'macro_f1': 0.97428404441846883,\n",
       "                             'micro_f1': 0.97727272727272729},\n",
       "                            {'macro_f1': 0.9642421779764323,\n",
       "                             'micro_f1': 0.96590909090909094},\n",
       "                            {'macro_f1': 0.97628032345013471,\n",
       "                             'micro_f1': 0.97727272727272729}],\n",
       "                           0.6: [{'macro_f1': 0.98856400259909027,\n",
       "                             'micro_f1': 0.98863636363636365},\n",
       "                            {'macro_f1': 0.9754464285714286,\n",
       "                             'micro_f1': 0.97727272727272729},\n",
       "                            {'macro_f1': 0.98725191945530932,\n",
       "                             'micro_f1': 0.98863636363636365}]})}})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fp = open('nclassv1.json', 'w')\n",
    "json.dump(results, fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
