{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inzamamrahaman/anaconda3/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['random']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "import dataloaders\n",
    "import models\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.metrics as metrics \n",
    "import sklearn.cluster as cluster\n",
    "import numpy as np \n",
    "import random\n",
    "import classifiers\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import sklearn.model_selection as model_selection\n",
    "import util\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/slashdot-cleaned.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-929001812148>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsplitDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/SignEmb-3985b33de9ca5751ec59cf7b3596bf12bb298a21 copy/src/dataloaders.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath, delimiter, ratio)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_assigner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_graph_data_as_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             self.train_X, self.test_X, self.train_y, self.test_y = train_test_split(self.X, self.y,\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/slashdot-cleaned.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = 'soc-sign-Slashdot081106'\n",
    "input_file = f'../data/{dataset}-cleaned.csv'\n",
    "delimiter = ','\n",
    "ratio = 0.8\n",
    "data = dataloaders.UnsplitDataset(filepath=input_file, ratio=ratio, delimiter=delimiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = data.get_shuffled_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters for PsuedoKernel \n",
    "num_nodes = data.get_num_nodes()\n",
    "dims = 16\n",
    "epochs = 100\n",
    "lr = 0.15\n",
    "lr_decay=0.0\n",
    "weight_decay=0.0\n",
    "lam = 0.00055\n",
    "p = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_fitter = models.fit_pseudo_kernel_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = model_selection.KFold(n_splits=num_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kf.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delta = 1\n",
    "delta0 = 0.5\n",
    "dims_array = [dims, 20, 20]\n",
    "frac1 = 1\n",
    "frac0 = 1\n",
    "p0 = True if frac0 > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================================\n",
      "========================          Fold #1                    ========================\n",
      "======================================================================================\n",
      "The loss at epoch  1  was  0.6923701763153076\n",
      "The loss at epoch  2  was  0.7004888653755188\n",
      "The loss at epoch  3  was  0.6118577122688293\n",
      "The loss at epoch  4  was  0.6119005680084229\n",
      "The loss at epoch  5  was  0.58583664894104\n",
      "The loss at epoch  6  was  0.5703405141830444\n",
      "The loss at epoch  7  was  0.5412038564682007\n",
      "The loss at epoch  8  was  0.5290126800537109\n",
      "The loss at epoch  9  was  0.5230953693389893\n",
      "The loss at epoch  10  was  0.5073431134223938\n",
      "The loss at epoch  11  was  0.49725544452667236\n",
      "The loss at epoch  12  was  0.477653443813324\n",
      "The loss at epoch  13  was  0.4491136372089386\n",
      "The loss at epoch  14  was  0.42958858609199524\n",
      "The loss at epoch  15  was  0.4406265616416931\n",
      "The loss at epoch  16  was  0.4756966829299927\n",
      "The loss at epoch  17  was  0.4155925512313843\n",
      "The loss at epoch  18  was  0.3996410667896271\n",
      "The loss at epoch  19  was  0.398274689912796\n",
      "The loss at epoch  20  was  0.3893606662750244\n",
      "The loss at epoch  21  was  0.3845425844192505\n",
      "The loss at epoch  22  was  0.38409844040870667\n",
      "The loss at epoch  23  was  0.3796377182006836\n",
      "The loss at epoch  24  was  0.3770451247692108\n",
      "The loss at epoch  25  was  0.37216684222221375\n",
      "The loss at epoch  26  was  0.3798918128013611\n",
      "The loss at epoch  27  was  0.3771449625492096\n",
      "The loss at epoch  28  was  0.37325388193130493\n",
      "The loss at epoch  29  was  0.37696805596351624\n",
      "The loss at epoch  30  was  0.37533941864967346\n",
      "The loss at epoch  31  was  0.37383371591567993\n",
      "The loss at epoch  32  was  0.37001049518585205\n",
      "The loss at epoch  33  was  0.37129974365234375\n",
      "The loss at epoch  34  was  0.36679908633232117\n",
      "The loss at epoch  35  was  0.3682089149951935\n",
      "The loss at epoch  36  was  0.37217333912849426\n",
      "The loss at epoch  37  was  0.3679922819137573\n",
      "The loss at epoch  38  was  0.3658711314201355\n",
      "The loss at epoch  39  was  0.36968958377838135\n",
      "The loss at epoch  40  was  0.36806562542915344\n",
      "The loss at epoch  41  was  0.366237074136734\n",
      "The loss at epoch  42  was  0.36701062321662903\n",
      "The loss at epoch  43  was  0.3686690926551819\n",
      "The loss at epoch  44  was  0.3650764226913452\n",
      "The loss at epoch  45  was  0.3645683825016022\n",
      "The loss at epoch  46  was  0.36946427822113037\n",
      "The loss at epoch  47  was  0.3699016571044922\n",
      "The loss at epoch  48  was  0.36343520879745483\n",
      "The loss at epoch  49  was  0.36435073614120483\n",
      "The loss at epoch  50  was  0.3643251061439514\n",
      "The loss at epoch  51  was  0.36304888129234314\n",
      "The loss at epoch  52  was  0.3595399260520935\n",
      "The loss at epoch  53  was  0.36224204301834106\n",
      "The loss at epoch  54  was  0.3638446033000946\n",
      "The loss at epoch  55  was  0.36811164021492004\n",
      "The loss at epoch  56  was  0.3599156141281128\n",
      "The loss at epoch  57  was  0.3611503839492798\n",
      "The loss at epoch  58  was  0.36232542991638184\n",
      "The loss at epoch  59  was  0.3600507974624634\n",
      "The loss at epoch  60  was  0.36205407977104187\n",
      "The loss at epoch  61  was  0.3612429201602936\n",
      "The loss at epoch  62  was  0.3584880530834198\n",
      "The loss at epoch  63  was  0.3615664839744568\n",
      "The loss at epoch  64  was  0.36129215359687805\n",
      "The loss at epoch  65  was  0.3601604998111725\n",
      "The loss at epoch  66  was  0.35714614391326904\n",
      "The loss at epoch  67  was  0.35908421874046326\n",
      "The loss at epoch  68  was  0.35722774267196655\n",
      "The loss at epoch  69  was  0.357028603553772\n",
      "The loss at epoch  70  was  0.3547728955745697\n",
      "The loss at epoch  71  was  0.35962796211242676\n",
      "The loss at epoch  72  was  0.3593529462814331\n",
      "The loss at epoch  73  was  0.3591246008872986\n",
      "The loss at epoch  74  was  0.3581983149051666\n",
      "The loss at epoch  75  was  0.3514954447746277\n",
      "The loss at epoch  76  was  0.3619450330734253\n",
      "The loss at epoch  77  was  0.356977641582489\n",
      "The loss at epoch  78  was  0.3552350699901581\n",
      "The loss at epoch  79  was  0.35785314440727234\n",
      "The loss at epoch  80  was  0.356934517621994\n",
      "The loss at epoch  81  was  0.35628458857536316\n",
      "The loss at epoch  82  was  0.3569117486476898\n",
      "The loss at epoch  83  was  0.36053505539894104\n",
      "The loss at epoch  84  was  0.3570118546485901\n",
      "The loss at epoch  85  was  0.35575953125953674\n",
      "The loss at epoch  86  was  0.36194345355033875\n",
      "The loss at epoch  87  was  0.35476645827293396\n",
      "The loss at epoch  88  was  0.35665372014045715\n",
      "The loss at epoch  89  was  0.3586108088493347\n",
      "The loss at epoch  90  was  0.35824495553970337\n",
      "The loss at epoch  91  was  0.3667818009853363\n",
      "The loss at epoch  92  was  0.35702741146087646\n",
      "The loss at epoch  93  was  0.3580632209777832\n",
      "The loss at epoch  94  was  0.3587964177131653\n",
      "The loss at epoch  95  was  0.3563064634799957\n",
      "The loss at epoch  96  was  0.35732752084732056\n",
      "The loss at epoch  97  was  0.36041122674942017\n",
      "The loss at epoch  98  was  0.354293555021286\n",
      "The loss at epoch  99  was  0.360237181186676\n",
      "The loss at epoch  100  was  0.3529585003852844\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Loss at epoch  1  is  0.9466879963874817\n",
      "Loss at epoch  2  is  0.8599691390991211\n",
      "Loss at epoch  3  is  0.14837411046028137\n",
      "Loss at epoch  4  is  0.8152338862419128\n",
      "Loss at epoch  5  is  0.16936473548412323\n",
      "Loss at epoch  6  is  0.16573625802993774\n",
      "Loss at epoch  7  is  0.16303369402885437\n",
      "Loss at epoch  8  is  0.15934708714485168\n",
      "Loss at epoch  9  is  0.15882399678230286\n",
      "Loss at epoch  10  is  0.1595120131969452\n",
      "Loss at epoch  11  is  0.15984830260276794\n",
      "Loss at epoch  12  is  0.1610969752073288\n",
      "Loss at epoch  13  is  0.15881359577178955\n",
      "Loss at epoch  14  is  0.15396848320960999\n",
      "Loss at epoch  15  is  0.1546448916196823\n",
      "Loss at epoch  16  is  0.15165390074253082\n",
      "Loss at epoch  17  is  0.1461896002292633\n",
      "Loss at epoch  18  is  0.15370410680770874\n",
      "Loss at epoch  19  is  0.1498580276966095\n",
      "Loss at epoch  20  is  0.14850811660289764\n",
      "Loss at epoch  21  is  0.13911129534244537\n",
      "Loss at epoch  22  is  0.12919312715530396\n",
      "Loss at epoch  23  is  0.12830738723278046\n",
      "Loss at epoch  24  is  0.12972958385944366\n",
      "Loss at epoch  25  is  0.12273038923740387\n",
      "Loss at epoch  26  is  0.113705113530159\n",
      "Loss at epoch  27  is  0.10925816744565964\n",
      "Loss at epoch  28  is  0.11783990263938904\n",
      "Loss at epoch  29  is  0.13716107606887817\n",
      "Loss at epoch  30  is  0.12103183567523956\n",
      "Loss at epoch  31  is  0.10815340280532837\n",
      "Loss at epoch  32  is  0.10839103162288666\n",
      "Loss at epoch  33  is  0.11216485500335693\n",
      "Loss at epoch  34  is  0.10631052404642105\n",
      "Loss at epoch  35  is  0.10521156340837479\n",
      "Loss at epoch  36  is  0.09879273921251297\n",
      "Loss at epoch  37  is  0.10277778655290604\n",
      "Loss at epoch  38  is  0.09823253750801086\n",
      "Loss at epoch  39  is  0.0978192463517189\n",
      "Loss at epoch  40  is  0.09630422294139862\n",
      "Loss at epoch  41  is  0.09513500332832336\n",
      "Loss at epoch  42  is  0.0910930410027504\n",
      "Loss at epoch  43  is  0.08921415358781815\n",
      "Loss at epoch  44  is  0.08701501786708832\n",
      "Loss at epoch  45  is  0.0865766778588295\n",
      "Loss at epoch  46  is  0.08805251121520996\n",
      "Loss at epoch  47  is  0.08895110338926315\n",
      "Loss at epoch  48  is  0.08538216352462769\n",
      "Loss at epoch  49  is  0.08738323301076889\n",
      "Loss at epoch  50  is  0.0851430669426918\n",
      "Loss at epoch  51  is  0.08604690432548523\n",
      "Loss at epoch  52  is  0.08306574821472168\n",
      "Loss at epoch  53  is  0.08222205191850662\n",
      "Loss at epoch  54  is  0.08241745829582214\n",
      "Loss at epoch  55  is  0.08873014897108078\n",
      "Loss at epoch  56  is  0.0856780856847763\n",
      "Loss at epoch  57  is  0.08457855880260468\n",
      "Loss at epoch  58  is  0.07931122928857803\n",
      "Loss at epoch  59  is  0.07781299203634262\n",
      "Loss at epoch  60  is  0.07739625126123428\n",
      "Loss at epoch  61  is  0.0804157704114914\n",
      "Loss at epoch  62  is  0.08093884587287903\n",
      "Loss at epoch  63  is  0.08796075731515884\n",
      "Loss at epoch  64  is  0.0820954218506813\n",
      "Loss at epoch  65  is  0.08347433805465698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch  66  is  0.07805003225803375\n",
      "Loss at epoch  67  is  0.0776064544916153\n",
      "Loss at epoch  68  is  0.07596553862094879\n",
      "Loss at epoch  69  is  0.0775417760014534\n",
      "Loss at epoch  70  is  0.07774101197719574\n",
      "Loss at epoch  71  is  0.0836687982082367\n",
      "Loss at epoch  72  is  0.07853498309850693\n",
      "Loss at epoch  73  is  0.07843071222305298\n",
      "Loss at epoch  74  is  0.07670009136199951\n",
      "Loss at epoch  75  is  0.07828227430582047\n",
      "Loss at epoch  76  is  0.07611235976219177\n",
      "Loss at epoch  77  is  0.07901444286108017\n",
      "Loss at epoch  78  is  0.0762544795870781\n",
      "Loss at epoch  79  is  0.07796601951122284\n",
      "Loss at epoch  80  is  0.07551892101764679\n",
      "Loss at epoch  81  is  0.07689239829778671\n",
      "Loss at epoch  82  is  0.07562108337879181\n",
      "Loss at epoch  83  is  0.07886097580194473\n",
      "Loss at epoch  84  is  0.07586626708507538\n",
      "Loss at epoch  85  is  0.07823013514280319\n",
      "Loss at epoch  86  is  0.07456376403570175\n",
      "Loss at epoch  87  is  0.07500364631414413\n",
      "Loss at epoch  88  is  0.07472602277994156\n",
      "Loss at epoch  89  is  0.08027663081884384\n",
      "Loss at epoch  90  is  0.07673833519220352\n",
      "Loss at epoch  91  is  0.07688750326633453\n",
      "Loss at epoch  92  is  0.07314643263816833\n",
      "Loss at epoch  93  is  0.0731084793806076\n",
      "Loss at epoch  94  is  0.07351988554000854\n",
      "Loss at epoch  95  is  0.08056725561618805\n",
      "Loss at epoch  96  is  0.07592328637838364\n",
      "Loss at epoch  97  is  0.07668958604335785\n",
      "Loss at epoch  98  is  0.07326865196228027\n",
      "Loss at epoch  99  is  0.07397729903459549\n",
      "Loss at epoch  100  is  0.07285087555646896\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "SVD error (low rank): 1.272523\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inzamamrahaman/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/inzamamrahaman/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:538: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n",
      "/Users/inzamamrahaman/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "======================================================================================\n",
      "========================          Fold #2                    ========================\n",
      "======================================================================================\n",
      "The loss at epoch  1  was  0.6949893832206726\n",
      "The loss at epoch  2  was  0.7083179950714111\n",
      "The loss at epoch  3  was  0.6805039644241333\n",
      "The loss at epoch  4  was  0.595696747303009\n",
      "The loss at epoch  5  was  0.5065411329269409\n",
      "The loss at epoch  6  was  0.46546128392219543\n",
      "The loss at epoch  7  was  0.4523494243621826\n",
      "The loss at epoch  8  was  0.4867177903652191\n",
      "The loss at epoch  9  was  0.4708542823791504\n",
      "The loss at epoch  10  was  0.4395187497138977\n",
      "The loss at epoch  11  was  0.4365807771682739\n",
      "The loss at epoch  12  was  0.4314687252044678\n",
      "The loss at epoch  13  was  0.42402568459510803\n",
      "The loss at epoch  14  was  0.4270978569984436\n",
      "The loss at epoch  15  was  0.4197167754173279\n",
      "The loss at epoch  16  was  0.4187691807746887\n",
      "The loss at epoch  17  was  0.41593801975250244\n",
      "The loss at epoch  18  was  0.417070209980011\n",
      "The loss at epoch  19  was  0.40783098340034485\n",
      "The loss at epoch  20  was  0.40570950508117676\n",
      "The loss at epoch  21  was  0.40790459513664246\n",
      "The loss at epoch  22  was  0.4109565317630768\n",
      "The loss at epoch  23  was  0.41011297702789307\n",
      "The loss at epoch  24  was  0.4059998393058777\n",
      "The loss at epoch  25  was  0.4009464383125305\n",
      "The loss at epoch  26  was  0.4007837474346161\n",
      "The loss at epoch  27  was  0.3995697796344757\n",
      "The loss at epoch  28  was  0.39861175417900085\n",
      "The loss at epoch  29  was  0.3980307877063751\n",
      "The loss at epoch  30  was  0.3970605731010437\n",
      "The loss at epoch  31  was  0.4003741443157196\n",
      "The loss at epoch  32  was  0.4008501172065735\n",
      "The loss at epoch  33  was  0.39260122179985046\n",
      "The loss at epoch  34  was  0.3874306380748749\n",
      "The loss at epoch  35  was  0.39036935567855835\n",
      "The loss at epoch  36  was  0.3869459629058838\n",
      "The loss at epoch  37  was  0.3874785900115967\n",
      "The loss at epoch  38  was  0.38600224256515503\n",
      "The loss at epoch  39  was  0.37886834144592285\n",
      "The loss at epoch  40  was  0.38171127438545227\n",
      "The loss at epoch  41  was  0.37950780987739563\n",
      "The loss at epoch  42  was  0.37822723388671875\n",
      "The loss at epoch  43  was  0.37825801968574524\n",
      "The loss at epoch  44  was  0.37554776668548584\n",
      "The loss at epoch  45  was  0.3747078776359558\n",
      "The loss at epoch  46  was  0.37365269660949707\n",
      "The loss at epoch  47  was  0.3724656105041504\n",
      "The loss at epoch  48  was  0.3738551735877991\n",
      "The loss at epoch  49  was  0.37282076478004456\n",
      "The loss at epoch  50  was  0.37528660893440247\n",
      "The loss at epoch  51  was  0.3713739514350891\n",
      "The loss at epoch  52  was  0.37479615211486816\n",
      "The loss at epoch  53  was  0.3701055943965912\n",
      "The loss at epoch  54  was  0.36664021015167236\n",
      "The loss at epoch  55  was  0.3700529634952545\n",
      "The loss at epoch  56  was  0.3652651011943817\n",
      "The loss at epoch  57  was  0.3695790767669678\n",
      "The loss at epoch  58  was  0.3723866939544678\n",
      "The loss at epoch  59  was  0.37131577730178833\n",
      "The loss at epoch  60  was  0.3674425482749939\n",
      "The loss at epoch  61  was  0.3643949031829834\n",
      "The loss at epoch  62  was  0.3637005686759949\n",
      "The loss at epoch  63  was  0.36537954211235046\n",
      "The loss at epoch  64  was  0.36564406752586365\n",
      "The loss at epoch  65  was  0.3660487234592438\n",
      "The loss at epoch  66  was  0.3653586208820343\n",
      "The loss at epoch  67  was  0.3610553741455078\n",
      "The loss at epoch  68  was  0.3605535626411438\n",
      "The loss at epoch  69  was  0.36181819438934326\n",
      "The loss at epoch  70  was  0.35765573382377625\n",
      "The loss at epoch  71  was  0.3625546097755432\n",
      "The loss at epoch  72  was  0.36116307973861694\n",
      "The loss at epoch  73  was  0.3648388683795929\n",
      "The loss at epoch  74  was  0.3625163435935974\n",
      "The loss at epoch  75  was  0.3610895872116089\n",
      "The loss at epoch  76  was  0.359844446182251\n",
      "The loss at epoch  77  was  0.36425861716270447\n",
      "The loss at epoch  78  was  0.3562445640563965\n",
      "The loss at epoch  79  was  0.3588428199291229\n",
      "The loss at epoch  80  was  0.3585372269153595\n",
      "The loss at epoch  81  was  0.36308807134628296\n",
      "The loss at epoch  82  was  0.36140286922454834\n",
      "The loss at epoch  83  was  0.3628356158733368\n",
      "The loss at epoch  84  was  0.36516469717025757\n",
      "The loss at epoch  85  was  0.3604402542114258\n",
      "The loss at epoch  86  was  0.35764163732528687\n",
      "The loss at epoch  87  was  0.3579045236110687\n",
      "The loss at epoch  88  was  0.35903629660606384\n",
      "The loss at epoch  89  was  0.35670676827430725\n",
      "The loss at epoch  90  was  0.3596815764904022\n",
      "The loss at epoch  91  was  0.3592756688594818\n",
      "The loss at epoch  92  was  0.35669025778770447\n",
      "The loss at epoch  93  was  0.35824185609817505\n",
      "The loss at epoch  94  was  0.3599861264228821\n",
      "The loss at epoch  95  was  0.35865139961242676\n",
      "The loss at epoch  96  was  0.3597225546836853\n",
      "The loss at epoch  97  was  0.3589988052845001\n",
      "The loss at epoch  98  was  0.36312368512153625\n",
      "The loss at epoch  99  was  0.35776960849761963\n",
      "The loss at epoch  100  was  0.35567668080329895\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Loss at epoch  1  is  0.9505782723426819\n",
      "Loss at epoch  2  is  0.843565046787262\n",
      "Loss at epoch  3  is  0.15487250685691833\n",
      "Loss at epoch  4  is  0.6868001222610474\n",
      "Loss at epoch  5  is  0.16545513272285461\n",
      "Loss at epoch  6  is  0.16407497227191925\n",
      "Loss at epoch  7  is  0.16198287904262543\n",
      "Loss at epoch  8  is  0.14872264862060547\n",
      "Loss at epoch  9  is  0.15707768499851227\n",
      "Loss at epoch  10  is  0.15589648485183716\n",
      "Loss at epoch  11  is  0.15523236989974976\n",
      "Loss at epoch  12  is  0.1530965268611908\n",
      "Loss at epoch  13  is  0.1558336317539215\n",
      "Loss at epoch  14  is  0.15626497566699982\n",
      "Loss at epoch  15  is  0.1540088951587677\n",
      "Loss at epoch  16  is  0.1557552069425583\n",
      "Loss at epoch  17  is  0.15452046692371368\n",
      "Loss at epoch  18  is  0.15497054159641266\n",
      "Loss at epoch  19  is  0.154568612575531\n",
      "Loss at epoch  20  is  0.1549074500799179\n",
      "Loss at epoch  21  is  0.1551019251346588\n",
      "Loss at epoch  22  is  0.155249685049057\n",
      "Loss at epoch  23  is  0.15338623523712158\n",
      "Loss at epoch  24  is  0.1538332849740982\n",
      "Loss at epoch  25  is  0.16187161207199097\n",
      "Loss at epoch  26  is  0.16075284779071808\n",
      "Loss at epoch  27  is  0.15151037275791168\n",
      "Loss at epoch  28  is  0.15075376629829407\n",
      "Loss at epoch  29  is  0.14972634613513947\n",
      "Loss at epoch  30  is  0.15011706948280334\n",
      "Loss at epoch  31  is  0.14916640520095825\n",
      "Loss at epoch  32  is  0.15499824285507202\n",
      "Loss at epoch  33  is  0.14927440881729126\n",
      "Loss at epoch  34  is  0.14829283952713013\n",
      "Loss at epoch  35  is  0.14920967817306519\n",
      "Loss at epoch  36  is  0.14806127548217773\n",
      "Loss at epoch  37  is  0.15040436387062073\n",
      "Loss at epoch  38  is  0.14834222197532654\n",
      "Loss at epoch  39  is  0.14801953732967377\n",
      "Loss at epoch  40  is  0.14742381870746613\n",
      "Loss at epoch  41  is  0.15139195322990417\n",
      "Loss at epoch  42  is  0.14882679283618927\n",
      "Loss at epoch  43  is  0.14730194211006165\n",
      "Loss at epoch  44  is  0.1465076357126236\n",
      "Loss at epoch  45  is  0.14559651911258698\n",
      "Loss at epoch  46  is  0.14964282512664795\n",
      "Loss at epoch  47  is  0.14422397315502167\n",
      "Loss at epoch  48  is  0.1446124166250229\n",
      "Loss at epoch  49  is  0.15008927881717682\n",
      "Loss at epoch  50  is  0.14400534331798553\n",
      "Loss at epoch  51  is  0.14482857286930084\n",
      "Loss at epoch  52  is  0.1489027887582779\n",
      "Loss at epoch  53  is  0.14563162624835968\n",
      "Loss at epoch  54  is  0.14062361419200897\n",
      "Loss at epoch  55  is  0.13908818364143372\n",
      "Loss at epoch  56  is  0.13797561824321747\n",
      "Loss at epoch  57  is  0.13810110092163086\n",
      "Loss at epoch  58  is  0.13772109150886536\n",
      "Loss at epoch  59  is  0.13410571217536926\n",
      "Loss at epoch  60  is  0.13316267728805542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch  61  is  0.13166497647762299\n",
      "Loss at epoch  62  is  0.12914995849132538\n",
      "Loss at epoch  63  is  0.12795531749725342\n",
      "Loss at epoch  64  is  0.12612010538578033\n",
      "Loss at epoch  65  is  0.12307778000831604\n",
      "Loss at epoch  66  is  0.11925691366195679\n",
      "Loss at epoch  67  is  0.11321517825126648\n",
      "Loss at epoch  68  is  0.1174321174621582\n",
      "Loss at epoch  69  is  0.11994430422782898\n",
      "Loss at epoch  70  is  0.10795328766107559\n",
      "Loss at epoch  71  is  0.10214865207672119\n",
      "Loss at epoch  72  is  0.10321459174156189\n",
      "Loss at epoch  73  is  0.10308320820331573\n",
      "Loss at epoch  74  is  0.10476462543010712\n",
      "Loss at epoch  75  is  0.10186634957790375\n",
      "Loss at epoch  76  is  0.10438886284828186\n",
      "Loss at epoch  77  is  0.09740442782640457\n",
      "Loss at epoch  78  is  0.0942833349108696\n",
      "Loss at epoch  79  is  0.09171956777572632\n",
      "Loss at epoch  80  is  0.09431913495063782\n",
      "Loss at epoch  81  is  0.0919705256819725\n",
      "Loss at epoch  82  is  0.09651537239551544\n",
      "Loss at epoch  83  is  0.0910446047782898\n",
      "Loss at epoch  84  is  0.09503833949565887\n",
      "Loss at epoch  85  is  0.0872768983244896\n",
      "Loss at epoch  86  is  0.0859154760837555\n",
      "Loss at epoch  87  is  0.08565475046634674\n",
      "Loss at epoch  88  is  0.09278985857963562\n",
      "Loss at epoch  89  is  0.08879102021455765\n",
      "Loss at epoch  90  is  0.09304781258106232\n",
      "Loss at epoch  91  is  0.08461816608905792\n",
      "Loss at epoch  92  is  0.08492636680603027\n",
      "Loss at epoch  93  is  0.08378486335277557\n",
      "Loss at epoch  94  is  0.08572174608707428\n",
      "Loss at epoch  95  is  0.08485017716884613\n",
      "Loss at epoch  96  is  0.0921439379453659\n",
      "Loss at epoch  97  is  0.08489564061164856\n",
      "Loss at epoch  98  is  0.08591842651367188\n",
      "Loss at epoch  99  is  0.08236078917980194\n",
      "Loss at epoch  100  is  0.08493165671825409\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "SVD error (low rank): 1.273954\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "======================================================================================\n",
      "========================          Fold #3                    ========================\n",
      "======================================================================================\n",
      "The loss at epoch  1  was  0.7008006572723389\n",
      "The loss at epoch  2  was  0.7119502425193787\n",
      "The loss at epoch  3  was  0.6846870183944702\n",
      "The loss at epoch  4  was  0.6067938208580017\n",
      "The loss at epoch  5  was  0.5760220289230347\n",
      "The loss at epoch  6  was  0.5526078939437866\n",
      "The loss at epoch  7  was  0.5194168090820312\n",
      "The loss at epoch  8  was  0.5070929527282715\n",
      "The loss at epoch  9  was  0.47979360818862915\n",
      "The loss at epoch  10  was  0.4528005123138428\n",
      "The loss at epoch  11  was  0.43378832936286926\n",
      "The loss at epoch  12  was  0.4324045181274414\n",
      "The loss at epoch  13  was  0.40587013959884644\n",
      "The loss at epoch  14  was  0.40458616614341736\n",
      "The loss at epoch  15  was  0.4160042405128479\n",
      "The loss at epoch  16  was  0.4269832968711853\n",
      "The loss at epoch  17  was  0.39601534605026245\n",
      "The loss at epoch  18  was  0.3874853849411011\n",
      "The loss at epoch  19  was  0.38052958250045776\n",
      "The loss at epoch  20  was  0.3848396837711334\n",
      "The loss at epoch  21  was  0.37916335463523865\n",
      "The loss at epoch  22  was  0.3758350610733032\n",
      "The loss at epoch  23  was  0.38043391704559326\n",
      "The loss at epoch  24  was  0.37700313329696655\n",
      "The loss at epoch  25  was  0.37530121207237244\n",
      "The loss at epoch  26  was  0.37663301825523376\n",
      "The loss at epoch  27  was  0.379024863243103\n",
      "The loss at epoch  28  was  0.36934566497802734\n",
      "The loss at epoch  29  was  0.36950916051864624\n",
      "The loss at epoch  30  was  0.37173888087272644\n",
      "The loss at epoch  31  was  0.370976984500885\n",
      "The loss at epoch  32  was  0.36694493889808655\n",
      "The loss at epoch  33  was  0.36848780512809753\n",
      "The loss at epoch  34  was  0.36754995584487915\n",
      "The loss at epoch  35  was  0.3682152032852173\n",
      "The loss at epoch  36  was  0.3638712167739868\n",
      "The loss at epoch  37  was  0.3669300675392151\n",
      "The loss at epoch  38  was  0.3623015284538269\n",
      "The loss at epoch  39  was  0.36387893557548523\n",
      "The loss at epoch  40  was  0.3693089485168457\n",
      "The loss at epoch  41  was  0.3641775846481323\n",
      "The loss at epoch  42  was  0.36276423931121826\n",
      "The loss at epoch  43  was  0.36219504475593567\n",
      "The loss at epoch  44  was  0.3599812686443329\n",
      "The loss at epoch  45  was  0.3632032573223114\n",
      "The loss at epoch  46  was  0.3631979525089264\n",
      "The loss at epoch  47  was  0.3650813400745392\n",
      "The loss at epoch  48  was  0.3636668622493744\n",
      "The loss at epoch  49  was  0.36223652958869934\n",
      "The loss at epoch  50  was  0.3641340136528015\n",
      "The loss at epoch  51  was  0.3650243282318115\n",
      "The loss at epoch  52  was  0.36054641008377075\n",
      "The loss at epoch  53  was  0.36124178767204285\n",
      "The loss at epoch  54  was  0.35843899846076965\n",
      "The loss at epoch  55  was  0.35872235894203186\n",
      "The loss at epoch  56  was  0.3611401319503784\n",
      "The loss at epoch  57  was  0.3618006408214569\n",
      "The loss at epoch  58  was  0.35994935035705566\n",
      "The loss at epoch  59  was  0.3630557954311371\n",
      "The loss at epoch  60  was  0.3586667478084564\n",
      "The loss at epoch  61  was  0.3594110906124115\n",
      "The loss at epoch  62  was  0.3600134253501892\n",
      "The loss at epoch  63  was  0.3602563738822937\n",
      "The loss at epoch  64  was  0.35793522000312805\n",
      "The loss at epoch  65  was  0.3605141043663025\n",
      "The loss at epoch  66  was  0.36240124702453613\n",
      "The loss at epoch  67  was  0.35579681396484375\n",
      "The loss at epoch  68  was  0.3531967103481293\n",
      "The loss at epoch  69  was  0.35736319422721863\n",
      "The loss at epoch  70  was  0.3569907546043396\n",
      "The loss at epoch  71  was  0.3597392439842224\n",
      "The loss at epoch  72  was  0.3559085726737976\n",
      "The loss at epoch  73  was  0.3566986322402954\n",
      "The loss at epoch  74  was  0.35672780871391296\n",
      "The loss at epoch  75  was  0.35760682821273804\n",
      "The loss at epoch  76  was  0.35666730999946594\n",
      "The loss at epoch  77  was  0.35752421617507935\n",
      "The loss at epoch  78  was  0.3576289415359497\n",
      "The loss at epoch  79  was  0.35631120204925537\n",
      "The loss at epoch  80  was  0.354678750038147\n",
      "The loss at epoch  81  was  0.3614913523197174\n",
      "The loss at epoch  82  was  0.3531792461872101\n",
      "The loss at epoch  83  was  0.3576997220516205\n",
      "The loss at epoch  84  was  0.35723215341567993\n",
      "The loss at epoch  85  was  0.3572555482387543\n",
      "The loss at epoch  86  was  0.3564172685146332\n",
      "The loss at epoch  87  was  0.35600000619888306\n",
      "The loss at epoch  88  was  0.3576303720474243\n",
      "The loss at epoch  89  was  0.3571246862411499\n",
      "The loss at epoch  90  was  0.35661840438842773\n",
      "The loss at epoch  91  was  0.3570979833602905\n",
      "The loss at epoch  92  was  0.3554176688194275\n",
      "The loss at epoch  93  was  0.3567151427268982\n",
      "The loss at epoch  94  was  0.35720914602279663\n",
      "The loss at epoch  95  was  0.3580613434314728\n",
      "The loss at epoch  96  was  0.3560838997364044\n",
      "The loss at epoch  97  was  0.35496407747268677\n",
      "The loss at epoch  98  was  0.35674113035202026\n",
      "The loss at epoch  99  was  0.36088472604751587\n",
      "The loss at epoch  100  was  0.35603299736976624\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Loss at epoch  1  is  0.9496528506278992\n",
      "Loss at epoch  2  is  0.8025112152099609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch  3  is  0.2718100845813751\n",
      "Loss at epoch  4  is  0.160152867436409\n",
      "Loss at epoch  5  is  0.18143340945243835\n",
      "Loss at epoch  6  is  0.16157712042331696\n",
      "Loss at epoch  7  is  0.16171175241470337\n",
      "Loss at epoch  8  is  0.1675826460123062\n",
      "Loss at epoch  9  is  0.16785083711147308\n",
      "Loss at epoch  10  is  0.16405729949474335\n",
      "Loss at epoch  11  is  0.15295001864433289\n",
      "Loss at epoch  12  is  0.16381125152111053\n",
      "Loss at epoch  13  is  0.1631615161895752\n",
      "Loss at epoch  14  is  0.16081906855106354\n",
      "Loss at epoch  15  is  0.1571255922317505\n",
      "Loss at epoch  16  is  0.1514555662870407\n",
      "Loss at epoch  17  is  0.14882278442382812\n",
      "Loss at epoch  18  is  0.14272040128707886\n",
      "Loss at epoch  19  is  0.14049144089221954\n",
      "Loss at epoch  20  is  0.14548490941524506\n",
      "Loss at epoch  21  is  0.14845222234725952\n",
      "Loss at epoch  22  is  0.14834371209144592\n",
      "Loss at epoch  23  is  0.144237220287323\n",
      "Loss at epoch  24  is  0.14261473715305328\n",
      "Loss at epoch  25  is  0.13807368278503418\n",
      "Loss at epoch  26  is  0.13804110884666443\n",
      "Loss at epoch  27  is  0.1335219144821167\n",
      "Loss at epoch  28  is  0.13067248463630676\n",
      "Loss at epoch  29  is  0.13241828978061676\n",
      "Loss at epoch  30  is  0.1491669863462448\n",
      "Loss at epoch  31  is  0.13777214288711548\n",
      "Loss at epoch  32  is  0.1359061598777771\n",
      "Loss at epoch  33  is  0.1292562633752823\n",
      "Loss at epoch  34  is  0.12735900282859802\n",
      "Loss at epoch  35  is  0.13007134199142456\n",
      "Loss at epoch  36  is  0.13951580226421356\n",
      "Loss at epoch  37  is  0.12638263404369354\n",
      "Loss at epoch  38  is  0.1223006621003151\n",
      "Loss at epoch  39  is  0.11918289959430695\n",
      "Loss at epoch  40  is  0.12754371762275696\n",
      "Loss at epoch  41  is  0.13553433120250702\n",
      "Loss at epoch  42  is  0.13776835799217224\n",
      "Loss at epoch  43  is  0.1237332671880722\n",
      "Loss at epoch  44  is  0.11956214904785156\n",
      "Loss at epoch  45  is  0.11988205462694168\n",
      "Loss at epoch  46  is  0.1275743842124939\n",
      "Loss at epoch  47  is  0.11918516457080841\n",
      "Loss at epoch  48  is  0.12206900119781494\n",
      "Loss at epoch  49  is  0.11698102951049805\n",
      "Loss at epoch  50  is  0.11913731694221497\n",
      "Loss at epoch  51  is  0.11498776078224182\n",
      "Loss at epoch  52  is  0.11612944304943085\n",
      "Loss at epoch  53  is  0.11467962712049484\n",
      "Loss at epoch  54  is  0.11854326725006104\n",
      "Loss at epoch  55  is  0.11729903519153595\n",
      "Loss at epoch  56  is  0.11749449372291565\n",
      "Loss at epoch  57  is  0.11049152910709381\n",
      "Loss at epoch  58  is  0.1091899573802948\n",
      "Loss at epoch  59  is  0.10981810092926025\n",
      "Loss at epoch  60  is  0.11101055145263672\n",
      "Loss at epoch  61  is  0.11259572207927704\n",
      "Loss at epoch  62  is  0.11391463875770569\n",
      "Loss at epoch  63  is  0.11231406778097153\n",
      "Loss at epoch  64  is  0.11110876500606537\n",
      "Loss at epoch  65  is  0.10866881161928177\n",
      "Loss at epoch  66  is  0.10758528113365173\n",
      "Loss at epoch  67  is  0.10617146641016006\n",
      "Loss at epoch  68  is  0.10678265243768692\n",
      "Loss at epoch  69  is  0.10898743569850922\n",
      "Loss at epoch  70  is  0.11267730593681335\n",
      "Loss at epoch  71  is  0.11410748213529587\n",
      "Loss at epoch  72  is  0.11658535897731781\n",
      "Loss at epoch  73  is  0.10889475792646408\n",
      "Loss at epoch  74  is  0.10614203661680222\n",
      "Loss at epoch  75  is  0.10428360849618912\n",
      "Loss at epoch  76  is  0.10385762155056\n",
      "Loss at epoch  77  is  0.10394170880317688\n",
      "Loss at epoch  78  is  0.10645893961191177\n",
      "Loss at epoch  79  is  0.1105000376701355\n",
      "Loss at epoch  80  is  0.11053505539894104\n",
      "Loss at epoch  81  is  0.10824880003929138\n",
      "Loss at epoch  82  is  0.10599591583013535\n",
      "Loss at epoch  83  is  0.10461758077144623\n",
      "Loss at epoch  84  is  0.10412228107452393\n",
      "Loss at epoch  85  is  0.10452812910079956\n",
      "Loss at epoch  86  is  0.10702311247587204\n",
      "Loss at epoch  87  is  0.10873188078403473\n",
      "Loss at epoch  88  is  0.10839112848043442\n",
      "Loss at epoch  89  is  0.10553918778896332\n",
      "Loss at epoch  90  is  0.1044619157910347\n",
      "Loss at epoch  91  is  0.10387727618217468\n",
      "Loss at epoch  92  is  0.10382877290248871\n",
      "Loss at epoch  93  is  0.10421961545944214\n",
      "Loss at epoch  94  is  0.10729977488517761\n",
      "Loss at epoch  95  is  0.10905860364437103\n",
      "Loss at epoch  96  is  0.10788702219724655\n",
      "Loss at epoch  97  is  0.1040981113910675\n",
      "Loss at epoch  98  is  0.10277897864580154\n",
      "Loss at epoch  99  is  0.10258166491985321\n",
      "Loss at epoch  100  is  0.10327613353729248\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "SVD error (low rank): 1.271736\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "======================================================================================\n",
      "========================          Fold #4                    ========================\n",
      "======================================================================================\n",
      "The loss at epoch  1  was  0.7042930126190186\n",
      "The loss at epoch  2  was  0.7151879072189331\n",
      "The loss at epoch  3  was  0.6899521350860596\n",
      "The loss at epoch  4  was  0.5885249376296997\n",
      "The loss at epoch  5  was  0.5583382248878479\n",
      "The loss at epoch  6  was  0.5147086381912231\n",
      "The loss at epoch  7  was  0.4975462555885315\n",
      "The loss at epoch  8  was  0.4950442314147949\n",
      "The loss at epoch  9  was  0.48639053106307983\n",
      "The loss at epoch  10  was  0.4826754629611969\n",
      "The loss at epoch  11  was  0.47560420632362366\n",
      "The loss at epoch  12  was  0.4785785973072052\n",
      "The loss at epoch  13  was  0.48375025391578674\n",
      "The loss at epoch  14  was  0.4796280264854431\n",
      "The loss at epoch  15  was  0.48552846908569336\n",
      "The loss at epoch  16  was  0.4701753258705139\n",
      "The loss at epoch  17  was  0.47120362520217896\n",
      "The loss at epoch  18  was  0.4700469374656677\n",
      "The loss at epoch  19  was  0.45940762758255005\n",
      "The loss at epoch  20  was  0.45961710810661316\n",
      "The loss at epoch  21  was  0.44161176681518555\n",
      "The loss at epoch  22  was  0.44429922103881836\n",
      "The loss at epoch  23  was  0.45291009545326233\n",
      "The loss at epoch  24  was  0.4393349885940552\n",
      "The loss at epoch  25  was  0.42857372760772705\n",
      "The loss at epoch  26  was  0.4191635847091675\n",
      "The loss at epoch  27  was  0.4141974151134491\n",
      "The loss at epoch  28  was  0.4168134331703186\n",
      "The loss at epoch  29  was  0.40950146317481995\n",
      "The loss at epoch  30  was  0.40706777572631836\n",
      "The loss at epoch  31  was  0.41152462363243103\n",
      "The loss at epoch  32  was  0.41671329736709595\n",
      "The loss at epoch  33  was  0.40965911746025085\n",
      "The loss at epoch  34  was  0.40876686573028564\n",
      "The loss at epoch  35  was  0.4024900197982788\n",
      "The loss at epoch  36  was  0.405872106552124\n",
      "The loss at epoch  37  was  0.4062037467956543\n",
      "The loss at epoch  38  was  0.4097525179386139\n",
      "The loss at epoch  39  was  0.3970155119895935\n",
      "The loss at epoch  40  was  0.3988155722618103\n",
      "The loss at epoch  41  was  0.39927005767822266\n",
      "The loss at epoch  42  was  0.39988085627555847\n",
      "The loss at epoch  43  was  0.4021548628807068\n",
      "The loss at epoch  44  was  0.4074786305427551\n",
      "The loss at epoch  45  was  0.4042845070362091\n",
      "The loss at epoch  46  was  0.4094789922237396\n",
      "The loss at epoch  47  was  0.41148924827575684\n",
      "The loss at epoch  48  was  0.40794169902801514\n",
      "The loss at epoch  49  was  0.4052375853061676\n",
      "The loss at epoch  50  was  0.4008365869522095\n",
      "The loss at epoch  51  was  0.4037835896015167\n",
      "The loss at epoch  52  was  0.4086666703224182\n",
      "The loss at epoch  53  was  0.40210074186325073\n",
      "The loss at epoch  54  was  0.4010741114616394\n",
      "The loss at epoch  55  was  0.3964548408985138\n",
      "The loss at epoch  56  was  0.3887411952018738\n",
      "The loss at epoch  57  was  0.3993246257305145\n",
      "The loss at epoch  58  was  0.4017113447189331\n",
      "The loss at epoch  59  was  0.40134695172309875\n",
      "The loss at epoch  60  was  0.404326856136322\n",
      "The loss at epoch  61  was  0.4021187424659729\n",
      "The loss at epoch  62  was  0.4042021632194519\n",
      "The loss at epoch  63  was  0.3922765552997589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss at epoch  64  was  0.3924233615398407\n",
      "The loss at epoch  65  was  0.3947916030883789\n",
      "The loss at epoch  66  was  0.39199596643447876\n",
      "The loss at epoch  67  was  0.3940446972846985\n",
      "The loss at epoch  68  was  0.39086514711380005\n",
      "The loss at epoch  69  was  0.3932468593120575\n",
      "The loss at epoch  70  was  0.396056205034256\n",
      "The loss at epoch  71  was  0.39353975653648376\n",
      "The loss at epoch  72  was  0.38731029629707336\n",
      "The loss at epoch  73  was  0.39329010248184204\n",
      "The loss at epoch  74  was  0.39234089851379395\n",
      "The loss at epoch  75  was  0.39174941182136536\n",
      "The loss at epoch  76  was  0.39300742745399475\n",
      "The loss at epoch  77  was  0.3900720179080963\n",
      "The loss at epoch  78  was  0.393748015165329\n",
      "The loss at epoch  79  was  0.39022520184516907\n",
      "The loss at epoch  80  was  0.3902828097343445\n",
      "The loss at epoch  81  was  0.38931190967559814\n",
      "The loss at epoch  82  was  0.39354655146598816\n",
      "The loss at epoch  83  was  0.3941497206687927\n",
      "The loss at epoch  84  was  0.3920454978942871\n",
      "The loss at epoch  85  was  0.3913257420063019\n",
      "The loss at epoch  86  was  0.3900814950466156\n",
      "The loss at epoch  87  was  0.3924028277397156\n",
      "The loss at epoch  88  was  0.39454999566078186\n",
      "The loss at epoch  89  was  0.38942596316337585\n",
      "The loss at epoch  90  was  0.3852842152118683\n",
      "The loss at epoch  91  was  0.3894513249397278\n",
      "The loss at epoch  92  was  0.39217430353164673\n",
      "The loss at epoch  93  was  0.389260858297348\n",
      "The loss at epoch  94  was  0.3841874897480011\n",
      "The loss at epoch  95  was  0.3900301158428192\n",
      "The loss at epoch  96  was  0.3938179612159729\n",
      "The loss at epoch  97  was  0.3884721100330353\n",
      "The loss at epoch  98  was  0.38980633020401\n",
      "The loss at epoch  99  was  0.3843560814857483\n",
      "The loss at epoch  100  was  0.3833935558795929\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Loss at epoch  1  is  0.9503613114356995\n",
      "Loss at epoch  2  is  0.8487460017204285\n",
      "Loss at epoch  3  is  0.20591816306114197\n",
      "Loss at epoch  4  is  0.24966831505298615\n",
      "Loss at epoch  5  is  0.19508209824562073\n",
      "Loss at epoch  6  is  0.17777466773986816\n",
      "Loss at epoch  7  is  0.1769193410873413\n",
      "Loss at epoch  8  is  0.171353280544281\n",
      "Loss at epoch  9  is  0.1676298826932907\n",
      "Loss at epoch  10  is  0.16337020695209503\n",
      "Loss at epoch  11  is  0.15723422169685364\n",
      "Loss at epoch  12  is  0.16500933468341827\n",
      "Loss at epoch  13  is  0.14540110528469086\n",
      "Loss at epoch  14  is  0.15602143108844757\n",
      "Loss at epoch  15  is  0.13850384950637817\n",
      "Loss at epoch  16  is  0.14728884398937225\n",
      "Loss at epoch  17  is  0.1387535035610199\n",
      "Loss at epoch  18  is  0.1346568763256073\n",
      "Loss at epoch  19  is  0.13597561419010162\n",
      "Loss at epoch  20  is  0.12907516956329346\n",
      "Loss at epoch  21  is  0.1261504590511322\n",
      "Loss at epoch  22  is  0.12301653623580933\n",
      "Loss at epoch  23  is  0.12326840311288834\n",
      "Loss at epoch  24  is  0.12114592641592026\n",
      "Loss at epoch  25  is  0.12471168488264084\n",
      "Loss at epoch  26  is  0.11773015558719635\n",
      "Loss at epoch  27  is  0.11548963189125061\n",
      "Loss at epoch  28  is  0.11435243487358093\n",
      "Loss at epoch  29  is  0.11146805435419083\n",
      "Loss at epoch  30  is  0.11251431703567505\n",
      "Loss at epoch  31  is  0.11109481751918793\n",
      "Loss at epoch  32  is  0.11099687218666077\n",
      "Loss at epoch  33  is  0.11221157014369965\n",
      "Loss at epoch  34  is  0.10940144956111908\n",
      "Loss at epoch  35  is  0.11119864881038666\n",
      "Loss at epoch  36  is  0.11261995136737823\n",
      "Loss at epoch  37  is  0.10796189308166504\n",
      "Loss at epoch  38  is  0.10600742697715759\n",
      "Loss at epoch  39  is  0.1055334210395813\n",
      "Loss at epoch  40  is  0.10540685057640076\n",
      "Loss at epoch  41  is  0.10494410991668701\n",
      "Loss at epoch  42  is  0.10561107099056244\n",
      "Loss at epoch  43  is  0.10304237902164459\n",
      "Loss at epoch  44  is  0.10231641680002213\n",
      "Loss at epoch  45  is  0.10634441673755646\n",
      "Loss at epoch  46  is  0.10413522273302078\n",
      "Loss at epoch  47  is  0.10179943591356277\n",
      "Loss at epoch  48  is  0.10205960273742676\n",
      "Loss at epoch  49  is  0.1004042699933052\n",
      "Loss at epoch  50  is  0.10429845750331879\n",
      "Loss at epoch  51  is  0.0994093045592308\n",
      "Loss at epoch  52  is  0.10062336176633835\n",
      "Loss at epoch  53  is  0.09758131206035614\n",
      "Loss at epoch  54  is  0.09839411824941635\n",
      "Loss at epoch  55  is  0.09750001132488251\n",
      "Loss at epoch  56  is  0.09662595391273499\n",
      "Loss at epoch  57  is  0.09962376952171326\n",
      "Loss at epoch  58  is  0.09555721282958984\n",
      "Loss at epoch  59  is  0.09534335881471634\n",
      "Loss at epoch  60  is  0.09447647631168365\n",
      "Loss at epoch  61  is  0.096124067902565\n",
      "Loss at epoch  62  is  0.0963120236992836\n",
      "Loss at epoch  63  is  0.0990428626537323\n",
      "Loss at epoch  64  is  0.0962287038564682\n",
      "Loss at epoch  65  is  0.099071204662323\n",
      "Loss at epoch  66  is  0.0949203297495842\n",
      "Loss at epoch  67  is  0.0944182276725769\n",
      "Loss at epoch  68  is  0.09413029998540878\n",
      "Loss at epoch  69  is  0.099903404712677\n",
      "Loss at epoch  70  is  0.0951189398765564\n",
      "Loss at epoch  71  is  0.09474186599254608\n",
      "Loss at epoch  72  is  0.09348347038030624\n",
      "Loss at epoch  73  is  0.09553110599517822\n",
      "Loss at epoch  74  is  0.09309335052967072\n",
      "Loss at epoch  75  is  0.09564673155546188\n",
      "Loss at epoch  76  is  0.0940837413072586\n",
      "Loss at epoch  77  is  0.09756045043468475\n",
      "Loss at epoch  78  is  0.09431150555610657\n",
      "Loss at epoch  79  is  0.09474971145391464\n",
      "Loss at epoch  80  is  0.09197717905044556\n",
      "Loss at epoch  81  is  0.09202857315540314\n",
      "Loss at epoch  82  is  0.09327514469623566\n",
      "Loss at epoch  83  is  0.09599793702363968\n",
      "Loss at epoch  84  is  0.09161319583654404\n",
      "Loss at epoch  85  is  0.09240180253982544\n",
      "Loss at epoch  86  is  0.09077835083007812\n",
      "Loss at epoch  87  is  0.09181258827447891\n",
      "Loss at epoch  88  is  0.09134771674871445\n",
      "Loss at epoch  89  is  0.09488508850336075\n",
      "Loss at epoch  90  is  0.09091060608625412\n",
      "Loss at epoch  91  is  0.0904865562915802\n",
      "Loss at epoch  92  is  0.08898366987705231\n",
      "Loss at epoch  93  is  0.0894838273525238\n",
      "Loss at epoch  94  is  0.0916324183344841\n",
      "Loss at epoch  95  is  0.09801933914422989\n",
      "Loss at epoch  96  is  0.09043209999799728\n",
      "Loss at epoch  97  is  0.0888700932264328\n",
      "Loss at epoch  98  is  0.08943308889865875\n",
      "Loss at epoch  99  is  0.09379763901233673\n",
      "Loss at epoch  100  is  0.0895804613828659\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "SVD error (low rank): 1.273154\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "======================================================================================\n",
      "========================          Fold #5                    ========================\n",
      "======================================================================================\n",
      "The loss at epoch  1  was  0.7027871608734131\n",
      "The loss at epoch  2  was  0.7111402153968811\n",
      "The loss at epoch  3  was  0.6328685283660889\n",
      "The loss at epoch  4  was  0.6154093742370605\n",
      "The loss at epoch  5  was  0.5825027227401733\n",
      "The loss at epoch  6  was  0.5621429681777954\n",
      "The loss at epoch  7  was  0.5437989234924316\n",
      "The loss at epoch  8  was  0.5209963917732239\n",
      "The loss at epoch  9  was  0.48908716440200806\n",
      "The loss at epoch  10  was  0.4683132469654083\n",
      "The loss at epoch  11  was  0.46280041337013245\n",
      "The loss at epoch  12  was  0.4512539207935333\n",
      "The loss at epoch  13  was  0.446097195148468\n",
      "The loss at epoch  14  was  0.4431069493293762\n",
      "The loss at epoch  15  was  0.43775153160095215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss at epoch  16  was  0.43492841720581055\n",
      "The loss at epoch  17  was  0.4286673069000244\n",
      "The loss at epoch  18  was  0.4214963912963867\n",
      "The loss at epoch  19  was  0.41217511892318726\n",
      "The loss at epoch  20  was  0.4174695312976837\n",
      "The loss at epoch  21  was  0.40234315395355225\n",
      "The loss at epoch  22  was  0.39763250946998596\n",
      "The loss at epoch  23  was  0.38937056064605713\n",
      "The loss at epoch  24  was  0.40709570050239563\n",
      "The loss at epoch  25  was  0.4140641689300537\n",
      "The loss at epoch  26  was  0.39705967903137207\n",
      "The loss at epoch  27  was  0.390484094619751\n",
      "The loss at epoch  28  was  0.3777211308479309\n",
      "The loss at epoch  29  was  0.37628206610679626\n",
      "The loss at epoch  30  was  0.37350404262542725\n",
      "The loss at epoch  31  was  0.37363505363464355\n",
      "The loss at epoch  32  was  0.37497755885124207\n",
      "The loss at epoch  33  was  0.3702617883682251\n",
      "The loss at epoch  34  was  0.3705168068408966\n",
      "The loss at epoch  35  was  0.36969107389450073\n",
      "The loss at epoch  36  was  0.36953550577163696\n",
      "The loss at epoch  37  was  0.37316209077835083\n",
      "The loss at epoch  38  was  0.36907801032066345\n",
      "The loss at epoch  39  was  0.36880403757095337\n",
      "The loss at epoch  40  was  0.3746712803840637\n",
      "The loss at epoch  41  was  0.36411669850349426\n",
      "The loss at epoch  42  was  0.3667628765106201\n",
      "The loss at epoch  43  was  0.36622291803359985\n",
      "The loss at epoch  44  was  0.3607819974422455\n",
      "The loss at epoch  45  was  0.3584766983985901\n",
      "The loss at epoch  46  was  0.36102592945098877\n",
      "The loss at epoch  47  was  0.3603251278400421\n",
      "The loss at epoch  48  was  0.3658873736858368\n",
      "The loss at epoch  49  was  0.3655470907688141\n",
      "The loss at epoch  50  was  0.35867854952812195\n",
      "The loss at epoch  51  was  0.3617040812969208\n",
      "The loss at epoch  52  was  0.35887062549591064\n",
      "The loss at epoch  53  was  0.36029374599456787\n",
      "The loss at epoch  54  was  0.35905179381370544\n",
      "The loss at epoch  55  was  0.3604073226451874\n",
      "The loss at epoch  56  was  0.3602848947048187\n",
      "The loss at epoch  57  was  0.36023426055908203\n",
      "The loss at epoch  58  was  0.36092954874038696\n",
      "The loss at epoch  59  was  0.3604378402233124\n",
      "The loss at epoch  60  was  0.3618146479129791\n",
      "The loss at epoch  61  was  0.36336255073547363\n",
      "The loss at epoch  62  was  0.3613307476043701\n",
      "The loss at epoch  63  was  0.35948896408081055\n",
      "The loss at epoch  64  was  0.3596547245979309\n",
      "The loss at epoch  65  was  0.3562518060207367\n",
      "The loss at epoch  66  was  0.3570243716239929\n",
      "The loss at epoch  67  was  0.35786738991737366\n",
      "The loss at epoch  68  was  0.3599773645401001\n",
      "The loss at epoch  69  was  0.3600892126560211\n",
      "The loss at epoch  70  was  0.35952135920524597\n",
      "The loss at epoch  71  was  0.35355955362319946\n",
      "The loss at epoch  72  was  0.3633616864681244\n",
      "The loss at epoch  73  was  0.35746705532073975\n",
      "The loss at epoch  74  was  0.3602478504180908\n",
      "The loss at epoch  75  was  0.3558999300003052\n",
      "The loss at epoch  76  was  0.35545504093170166\n",
      "The loss at epoch  77  was  0.36011916399002075\n",
      "The loss at epoch  78  was  0.3626560866832733\n",
      "The loss at epoch  79  was  0.35755571722984314\n",
      "The loss at epoch  80  was  0.3590855896472931\n",
      "The loss at epoch  81  was  0.36164388060569763\n",
      "The loss at epoch  82  was  0.3565561771392822\n",
      "The loss at epoch  83  was  0.35561102628707886\n",
      "The loss at epoch  84  was  0.352817177772522\n",
      "The loss at epoch  85  was  0.35242465138435364\n",
      "The loss at epoch  86  was  0.35543298721313477\n",
      "The loss at epoch  87  was  0.35925552248954773\n",
      "The loss at epoch  88  was  0.3574373722076416\n",
      "The loss at epoch  89  was  0.35410076379776\n",
      "The loss at epoch  90  was  0.35497981309890747\n",
      "The loss at epoch  91  was  0.35829368233680725\n",
      "The loss at epoch  92  was  0.3556901514530182\n",
      "The loss at epoch  93  was  0.35527846217155457\n",
      "The loss at epoch  94  was  0.36022186279296875\n",
      "The loss at epoch  95  was  0.35495859384536743\n",
      "The loss at epoch  96  was  0.3553204834461212\n",
      "The loss at epoch  97  was  0.3546689748764038\n",
      "The loss at epoch  98  was  0.3546344041824341\n",
      "The loss at epoch  99  was  0.3567298948764801\n",
      "The loss at epoch  100  was  0.3553379476070404\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Loss at epoch  1  is  0.9456461071968079\n",
      "Loss at epoch  2  is  0.9106425046920776\n",
      "Loss at epoch  3  is  0.161187544465065\n",
      "Loss at epoch  4  is  0.7073737382888794\n",
      "Loss at epoch  5  is  0.19785991311073303\n",
      "Loss at epoch  6  is  0.18773987889289856\n",
      "Loss at epoch  7  is  0.17250263690948486\n",
      "Loss at epoch  8  is  0.1674680858850479\n",
      "Loss at epoch  9  is  0.15591107308864594\n",
      "Loss at epoch  10  is  0.15631060302257538\n",
      "Loss at epoch  11  is  0.16310159862041473\n",
      "Loss at epoch  12  is  0.14580531418323517\n",
      "Loss at epoch  13  is  0.1286923736333847\n",
      "Loss at epoch  14  is  0.13882189989089966\n",
      "Loss at epoch  15  is  0.12983985245227814\n",
      "Loss at epoch  16  is  0.12612910568714142\n",
      "Loss at epoch  17  is  0.12428226321935654\n",
      "Loss at epoch  18  is  0.14375445246696472\n",
      "Loss at epoch  19  is  0.13702689111232758\n",
      "Loss at epoch  20  is  0.12066924571990967\n",
      "Loss at epoch  21  is  0.11190622299909592\n",
      "Loss at epoch  22  is  0.10544827580451965\n",
      "Loss at epoch  23  is  0.10234296321868896\n",
      "Loss at epoch  24  is  0.10289402306079865\n",
      "Loss at epoch  25  is  0.1127006933093071\n",
      "Loss at epoch  26  is  0.1124504804611206\n",
      "Loss at epoch  27  is  0.11231154948472977\n",
      "Loss at epoch  28  is  0.10107187926769257\n",
      "Loss at epoch  29  is  0.10238246619701385\n",
      "Loss at epoch  30  is  0.09640353918075562\n",
      "Loss at epoch  31  is  0.0941796600818634\n",
      "Loss at epoch  32  is  0.09294687211513519\n",
      "Loss at epoch  33  is  0.09941206872463226\n",
      "Loss at epoch  34  is  0.09706079214811325\n",
      "Loss at epoch  35  is  0.10001736134290695\n",
      "Loss at epoch  36  is  0.09185892343521118\n",
      "Loss at epoch  37  is  0.09061077982187271\n",
      "Loss at epoch  38  is  0.08862906694412231\n",
      "Loss at epoch  39  is  0.09077181667089462\n",
      "Loss at epoch  40  is  0.09031961858272552\n",
      "Loss at epoch  41  is  0.09803073108196259\n",
      "Loss at epoch  42  is  0.0907689556479454\n",
      "Loss at epoch  43  is  0.09062529355287552\n",
      "Loss at epoch  44  is  0.08662494271993637\n",
      "Loss at epoch  45  is  0.08680875599384308\n",
      "Loss at epoch  46  is  0.08562130481004715\n",
      "Loss at epoch  47  is  0.0895194411277771\n",
      "Loss at epoch  48  is  0.08776912093162537\n",
      "Loss at epoch  49  is  0.09296665340662003\n",
      "Loss at epoch  50  is  0.08727908134460449\n",
      "Loss at epoch  51  is  0.08760321885347366\n",
      "Loss at epoch  52  is  0.08442094922065735\n",
      "Loss at epoch  53  is  0.08403949439525604\n",
      "Loss at epoch  54  is  0.08427029848098755\n",
      "Loss at epoch  55  is  0.08803079277276993\n",
      "Loss at epoch  56  is  0.08531830459833145\n",
      "Loss at epoch  57  is  0.08754393458366394\n",
      "Loss at epoch  58  is  0.08449170738458633\n",
      "Loss at epoch  59  is  0.08669300377368927\n",
      "Loss at epoch  60  is  0.08408527821302414\n",
      "Loss at epoch  61  is  0.08545230329036713\n",
      "Loss at epoch  62  is  0.08325184881687164\n",
      "Loss at epoch  63  is  0.08492103964090347\n",
      "Loss at epoch  64  is  0.08436509221792221\n",
      "Loss at epoch  65  is  0.08804906904697418\n",
      "Loss at epoch  66  is  0.08469225466251373\n",
      "Loss at epoch  67  is  0.08607479929924011\n",
      "Loss at epoch  68  is  0.08206025511026382\n",
      "Loss at epoch  69  is  0.08222238719463348\n",
      "Loss at epoch  70  is  0.08094263076782227\n",
      "Loss at epoch  71  is  0.08445808291435242\n",
      "Loss at epoch  72  is  0.08418911695480347\n",
      "Loss at epoch  73  is  0.08587494492530823\n",
      "Loss at epoch  74  is  0.08390621840953827\n",
      "Loss at epoch  75  is  0.08503873646259308\n",
      "Loss at epoch  76  is  0.08218088746070862\n",
      "Loss at epoch  77  is  0.08238915354013443\n",
      "Loss at epoch  78  is  0.08046586811542511\n",
      "Loss at epoch  79  is  0.08264300972223282\n",
      "Loss at epoch  80  is  0.0828070193529129\n",
      "Loss at epoch  81  is  0.08540728688240051\n",
      "Loss at epoch  82  is  0.08254234492778778\n",
      "Loss at epoch  83  is  0.0821024551987648\n",
      "Loss at epoch  84  is  0.08009837567806244\n",
      "Loss at epoch  85  is  0.08066758513450623\n",
      "Loss at epoch  86  is  0.08103074133396149\n",
      "Loss at epoch  87  is  0.08357134461402893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch  88  is  0.08295494318008423\n",
      "Loss at epoch  89  is  0.08321475982666016\n",
      "Loss at epoch  90  is  0.08000870048999786\n",
      "Loss at epoch  91  is  0.07964356243610382\n",
      "Loss at epoch  92  is  0.07899083942174911\n",
      "Loss at epoch  93  is  0.08131396025419235\n",
      "Loss at epoch  94  is  0.08169826865196228\n",
      "Loss at epoch  95  is  0.08455338329076767\n",
      "Loss at epoch  96  is  0.08314171433448792\n",
      "Loss at epoch  97  is  0.083824522793293\n",
      "Loss at epoch  98  is  0.08019349724054337\n",
      "Loss at epoch  99  is  0.0790279284119606\n",
      "Loss at epoch  100  is  0.07874997705221176\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "SVD error (low rank): 1.273938\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n",
      "Assembling training set features....\n",
      "Fitting classifier model\n",
      "Assembling testing set features\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reports_pksem = []\n",
    "reports_sine = []\n",
    "reports_hope = []\n",
    "#operation = 'hadamard'\n",
    "operations = ['hadamard', 'l1', 'l2', 'concat', 'average']\n",
    "reports_pksem = defaultdict(list)\n",
    "reports_sine = defaultdict(list)\n",
    "reports_hope = defaultdict(list)\n",
    "count = 1\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    \n",
    "    print(\"======================================================================================\")\n",
    "    print(f\"========================          Fold #{count}                    ========================\")\n",
    "    print(\"======================================================================================\")\n",
    "    \n",
    "    \n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_test, y_test = X[test_idx], y[test_idx]\n",
    "    \n",
    "    # train and evaluate pksem\n",
    "    kernel_model = models.fit_pseudo_kernel_model(num_nodes, dims, X, y, epochs=epochs, p=p, \n",
    "                                              lr=lr,lr_decay=lr_decay, lam=lam, weight_decay=weight_decay, \n",
    "                                                  undersample=True)\n",
    "    \n",
    "    for operation in operations:\n",
    "        clf_pksem = linear_model.LogisticRegression()\n",
    "        report = classifiers.train_and_evaluate_classifier(clf_pksem, kernel_model, X_train, y_train, X_test, y_test,\n",
    "                                                    operation=operation, undersample=True, ratio=2)\n",
    "        reports_pksem[operation].append(report)\n",
    "    \n",
    "    # train and evaluate SiNE\n",
    "    triples, triples0 = util.triples_from_array(X_train, y_train)\n",
    "    batch_size = int(frac1 * len(triples))\n",
    "    batch_size0 = int(frac0 * len(triples0))\n",
    "    sine_model = models.fit_sine_model(num_nodes, dims_array, triples, triples0, delta, delta0,batch_size, batch_size0, epochs, lr=lr, lr_decay=lr_decay,lam=lam, p=p, p0=p0)\n",
    "    for operation in operations:\n",
    "        clf_sine = linear_model.LogisticRegression()\n",
    "        report = classifiers.train_and_evaluate_classifier(clf_sine, sine_model, X_train, y_train, X_test, y_test,\n",
    "                                                    operation=operation, undersample=True, ratio=2)\n",
    "        reports_sine[operation].append(report)\n",
    "        \n",
    "    # train and evaluate hope\n",
    "    hope_model = models.fit_hope(dims, X_train, num_nodes)\n",
    "    for operation in operations:\n",
    "        clf_hope = linear_model.LogisticRegression()\n",
    "        report = classifiers.train_and_evaluate_classifier(clf_hope, hope_model, X_train, y_train, X_test, y_test,\n",
    "                                                    operation=operation, undersample=True, ratio=2)\n",
    "        reports_hope[operation].append(report)    \n",
    "    count += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'auc': 0.78764715637284122, 'average_percision_score': 0.97318780300475427, 'macro_f1': 0.65185885686223199, 'micro_f1': 0.89204499080097999, 'kappa': 0.30835945129496994, 'mathew': 0.32020841293158464}, {'auc': 0.013864954181544653, 'average_percision_score': 0.0022808461720743578, 'macro_f1': 0.023444458606964907, 'micro_f1': 0.01617826083542034, 'kappa': 0.043136749299164931, 'mathew': 0.037424568717141657})\n",
      "({'auc': 0.94973301965580714, 'average_percision_score': 0.99559593284947867, 'macro_f1': 0.76946351805086799, 'micro_f1': 0.92119338470517553, 'kappa': 0.54430473935139756, 'mathew': 0.58223887068291336}, {'auc': 0.013277928535907622, 'average_percision_score': 0.0010315744130060321, 'macro_f1': 0.038028748466332064, 'micro_f1': 0.013468540519935349, 'kappa': 0.073813262935287671, 'mathew': 0.069007922308597938})\n",
      "({'auc': 0.74265563864953044, 'average_percision_score': 0.9749799077227671, 'macro_f1': 0.5607778291773321, 'micro_f1': 0.93649263638261737, 'kappa': 0.13759274535010044, 'mathew': 0.19525383332510932}, {'auc': 0.0086517509435316258, 'average_percision_score': 0.0013773449290477228, 'macro_f1': 0.01571675121788297, 'micro_f1': 0.0056170435828641774, 'kappa': 0.031361901508588218, 'mathew': 0.046875939491830526})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def average_scores(reports, scores=['auc', 'average_percision_score', 'macro_f1', 'micro_f1', \n",
    "                                   'kappa', 'mathew']):\n",
    "    score_lst = dict()\n",
    "    for key in scores:\n",
    "        score_lst[key] = []\n",
    "    for report in reports:\n",
    "        for key in scores:\n",
    "            score_lst[key].append(report[key])\n",
    "    \n",
    "    score_avg = dict()\n",
    "    score_std = dict()\n",
    "    for key in scores:\n",
    "        score_avg[key] = np.mean(score_lst[key])\n",
    "        score_std[key] = np.std(score_lst[key])\n",
    "        \n",
    "    return score_avg, score_std\n",
    "print(average_scores(reports_sine['concat']))\n",
    "print(average_scores(reports_pksem['concat']))\n",
    "print(average_scores(reports_hope['concat']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_scores(reports, scores=['auc', 'average_percision_score', 'macro_f1', 'micro_f1', \n",
    "                                   'kappa', 'mathew']):\n",
    "    score_lst = dict()\n",
    "    for key in scores:\n",
    "        score_lst[key] = []\n",
    "    for report in reports:\n",
    "        for key in scores:\n",
    "            score_lst[key].append(report[key])\n",
    "    \n",
    "    score_avg = dict()\n",
    "    score_std = dict()\n",
    "    for key in scores:\n",
    "        score_avg[key] = np.mean(score_lst[key])\n",
    "        score_std[key] = np.std(score_lst[key])\n",
    "        \n",
    "    return score_avg, score_std\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_link_pred_file = f'link-predv3?{dataset}.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(result_link_pred_file, 'a') as fp:\n",
    "    for operation in operations:\n",
    "        r1 = average_scores(reports_pksem[operation])\n",
    "        r2 = average_scores(reports_sine[operation])\n",
    "        r3 = average_scores(reports_hope[operation])\n",
    "        \n",
    "        line = '{0},{1},{2},{3},{4},{5},{6},{7}\\n\\n'.format(input_file, \n",
    "                                                            operation, r1[0], r1[1], \n",
    "                                                            r2[0], r2[1],\n",
    "                                                            r3[0], r3[1])\n",
    "        fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.758396533044421"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(0.923 - 0.930)/0.923 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
